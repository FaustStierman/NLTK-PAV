{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07bbf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(txt):\n",
    "    text = ''\n",
    "    for line in txt:\n",
    "\n",
    "        text += line.replace('\\n',' ').replace('\\\"', '').replace('!','.').replace('?','.').replace(',','')\n",
    "    sentences = text.split('.')\n",
    "    sentences = [x.lower().strip(string.whitespace) for x in sentences]\n",
    "    tokenized = [re.findall(r'[\\w]+',sentence) for sentence in sentences]\n",
    "    tokens = [item for sublist in tokenized for item in sublist]\n",
    "    return (sentences, tokenized, tokens)\n",
    "\n",
    "\n",
    "def text_data(text):\n",
    "    \n",
    "    tk = LegalitySyllableTokenizer(words.words())\n",
    "    sentence_count = len(text)\n",
    "    word_count = sum([len(s) for s in text])\n",
    "    syllable_count = sum([sum([len(tk.tokenize(w)) for w in s]) for s in text])\n",
    "    \n",
    "    return (sentence_count, word_count, syllable_count)\n",
    "\n",
    "def Flesch_Kincaid(sent, words, sylab):\n",
    "    print(sent, words,sylab)\n",
    "    return 206.853 - 1.015* (words/sent) - 84.6 * (sylab/words)\n",
    "\n",
    "def Dale_Chall(sent, words):\n",
    "    \n",
    "    easywords = set()\n",
    "    \n",
    "    with open('DaleChallEasyWordList.txt', 'r') as f:\n",
    "        for w in f:\n",
    "            easywords.add(w.strip(string.whitespace))\n",
    "    \n",
    "    hard_words = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for s in words:\n",
    "        if s not in easywords:\n",
    "            hard_words += 1\n",
    "\n",
    "        total_words += 1\n",
    "\n",
    "    return 100 - (0.1579 * ((hard_words)/ total_words)*100 + 0.0469 * (total_words/sent))* 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9266e045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 722 1445\n",
      "FK aisimp:  20.493267345229697\n",
      "DC aisimp:  22.79787747213811\n",
      "\n",
      "\n",
      "27 628 1275\n",
      "FK lingsimp:  11.485297711724513\n",
      "DC lingsimp:  33.77612078320358\n",
      "\n",
      "\n",
      "425 9136 19055\n",
      "FK ai:  8.58344750180288\n",
      "DC ai:  12.610066414958268\n",
      "\n",
      "\n",
      "293 6874 14917\n",
      "FK ling:  -0.546831158810761\n",
      "DC ling:  20.728243316806356\n",
      "\n",
      "\n",
      "50 234 476\n",
      "FK gen:  30.010492307692346\n",
      "DC gen:  56.64311418803418\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = [(aisimptokens,aisimptoken,'aisimp'),(lingsimptokens,lingsimptoken,'lingsimp'),(aitokens,aitoken,'ai'),\n",
    "     (lingtokens,lingtoken,'ling'),\n",
    "    (gen,[item for sublist in gen for item in sublist], 'gen')]\n",
    "\n",
    "for i in a:\n",
    "    sent, w, syl = text_data(i[0])\n",
    "    print(f'FK {i[2]}: ',Flesch_Kincaid(sent, w, syl))\n",
    "    print(f'DC {i[2]}: ',Dale_Chall(sent,i[1]))\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45aa70f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study',\n",
       " 'find',\n",
       " 'both',\n",
       " 'own',\n",
       " 'meaning',\n",
       " 'study',\n",
       " 'find',\n",
       " 'both',\n",
       " 'computational',\n",
       " 'field',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'similar',\n",
       " 'panini',\n",
       " 'study',\n",
       " 'find',\n",
       " 'programme',\n",
       " 'linguists',\n",
       " 'study',\n",
       " 'find',\n",
       " 'language',\n",
       " 'india',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'monthlong',\n",
       " 'meaning',\n",
       " 'study',\n",
       " 'find',\n",
       " 'some',\n",
       " 'many',\n",
       " 'investigations',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'many',\n",
       " 'past',\n",
       " 'study',\n",
       " 'find',\n",
       " 'every',\n",
       " 'linguistic',\n",
       " 'linguistics',\n",
       " 'study',\n",
       " 'find',\n",
       " 'some',\n",
       " 'forensic',\n",
       " 'example',\n",
       " 'study',\n",
       " 'find',\n",
       " 'both',\n",
       " 'western',\n",
       " 'danger',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'human',\n",
       " 'panini',\n",
       " 'study',\n",
       " 'find',\n",
       " 'some',\n",
       " 'artificial',\n",
       " 'society',\n",
       " 'study',\n",
       " 'find',\n",
       " 'a',\n",
       " 'smart',\n",
       " 'structured',\n",
       " 'study',\n",
       " 'find',\n",
       " 'a',\n",
       " 'theoretical',\n",
       " 'pace',\n",
       " 'study',\n",
       " 'find',\n",
       " 'some',\n",
       " 'current',\n",
       " 'linguists',\n",
       " 'study',\n",
       " 'find',\n",
       " 'analysis',\n",
       " 'term',\n",
       " 'study',\n",
       " 'find',\n",
       " 'humanity',\n",
       " 'discourse',\n",
       " 'study',\n",
       " 'find',\n",
       " 'work',\n",
       " 'sociolinguistics',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'human',\n",
       " 'programme',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'literary',\n",
       " 'philosopher',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'theoretical',\n",
       " 'time',\n",
       " 'study',\n",
       " 'find',\n",
       " 'every',\n",
       " 'forensic',\n",
       " 'word',\n",
       " 'study',\n",
       " 'find',\n",
       " 'both',\n",
       " 'sanskrit',\n",
       " 'intelligence',\n",
       " 'study',\n",
       " 'find',\n",
       " 'both',\n",
       " 'theoretical',\n",
       " 'philosopher',\n",
       " 'study',\n",
       " 'find',\n",
       " 'history',\n",
       " 'society',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'smart',\n",
       " 'time',\n",
       " 'study',\n",
       " 'find',\n",
       " 'a',\n",
       " 'own',\n",
       " 'name',\n",
       " 'study',\n",
       " 'find',\n",
       " 'term',\n",
       " 'use',\n",
       " 'study',\n",
       " 'find',\n",
       " 'both',\n",
       " 'literary',\n",
       " 'grammar',\n",
       " 'study',\n",
       " 'find',\n",
       " 'every',\n",
       " 'time',\n",
       " 'study',\n",
       " 'find',\n",
       " 'use',\n",
       " 'semiotics',\n",
       " 'study',\n",
       " 'find',\n",
       " 'both',\n",
       " 'computational',\n",
       " 'programme',\n",
       " 'study',\n",
       " 'find',\n",
       " 'plato',\n",
       " 'society',\n",
       " 'study',\n",
       " 'find',\n",
       " 'some',\n",
       " 'things',\n",
       " 'study',\n",
       " 'find',\n",
       " 'a',\n",
       " 'own',\n",
       " 'world',\n",
       " 'study',\n",
       " 'find',\n",
       " 'both',\n",
       " 'first',\n",
       " 'india',\n",
       " 'study',\n",
       " 'find',\n",
       " 'a',\n",
       " 'general',\n",
       " 'example',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'many',\n",
       " 'bc',\n",
       " 'study',\n",
       " 'find',\n",
       " 'every',\n",
       " 'competencies',\n",
       " 'study',\n",
       " 'find',\n",
       " 'every',\n",
       " 'little',\n",
       " 'intelligence',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'smart',\n",
       " 'etymology',\n",
       " 'study',\n",
       " 'find',\n",
       " 'india',\n",
       " 'study',\n",
       " 'find',\n",
       " 'some',\n",
       " 'applied',\n",
       " 'grammar',\n",
       " 'study',\n",
       " 'find',\n",
       " 'a',\n",
       " 'western',\n",
       " 'term',\n",
       " 'study',\n",
       " 'find',\n",
       " 'a',\n",
       " 'hard',\n",
       " 'intelligence',\n",
       " 'study',\n",
       " 'find',\n",
       " 'linguistics',\n",
       " 'structured',\n",
       " 'study',\n",
       " 'find',\n",
       " 'time',\n",
       " 'plato',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'historical',\n",
       " 'history',\n",
       " 'study',\n",
       " 'find',\n",
       " 'the',\n",
       " 'computational',\n",
       " 'intelligence']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = [['study', 'find', 'both', 'own', 'meaning'],\n",
    " ['study', 'find', 'both', 'computational', 'field'],\n",
    " ['study', 'find', 'the', 'similar', 'panini'],\n",
    " ['study', 'find', 'programme', 'linguists'],\n",
    " ['study', 'find', 'language', 'india'],\n",
    " ['study', 'find', 'the', 'monthlong', 'meaning'],\n",
    " ['study', 'find', 'some', 'many', 'investigations'],\n",
    " ['study', 'find', 'the', 'many', 'past'],\n",
    " ['study', 'find', 'every', 'linguistic', 'linguistics'],\n",
    " ['study', 'find', 'some', 'forensic', 'example'],\n",
    " ['study', 'find', 'both', 'western', 'danger'],\n",
    " ['study', 'find', 'the', 'human', 'panini'],\n",
    " ['study', 'find', 'some', 'artificial', 'society'],\n",
    " ['study', 'find', 'a', 'smart', 'structured'],\n",
    " ['study', 'find', 'a', 'theoretical', 'pace'],\n",
    " ['study', 'find', 'some', 'current', 'linguists'],\n",
    " ['study', 'find', 'analysis', 'term'],\n",
    " ['study', 'find', 'humanity', 'discourse'],\n",
    " ['study', 'find', 'work', 'sociolinguistics'],\n",
    " ['study', 'find', 'the', 'human', 'programme'],\n",
    " ['study', 'find', 'the', 'literary', 'philosopher'],\n",
    " ['study', 'find', 'the', 'theoretical', 'time'],\n",
    " ['study', 'find', 'every', 'forensic', 'word'],\n",
    " ['study', 'find', 'both', 'sanskrit', 'intelligence'],\n",
    " ['study', 'find', 'both', 'theoretical', 'philosopher'],\n",
    " ['study', 'find', 'history', 'society'],\n",
    " ['study', 'find', 'the', 'smart', 'time'],\n",
    " ['study', 'find', 'a', 'own', 'name'],\n",
    " ['study', 'find', 'term', 'use'],\n",
    " ['study', 'find', 'both', 'literary', 'grammar'],\n",
    " ['study', 'find', 'every', 'time'],\n",
    " ['study', 'find', 'use', 'semiotics'],\n",
    " ['study', 'find', 'both', 'computational', 'programme'],\n",
    " ['study', 'find', 'plato', 'society'],\n",
    " ['study', 'find', 'some', 'things'],\n",
    " ['study', 'find', 'a', 'own', 'world'],\n",
    " ['study', 'find', 'both', 'first', 'india'],\n",
    " ['study', 'find', 'a', 'general', 'example'],\n",
    " ['study', 'find', 'the', 'many', 'bc'],\n",
    " ['study', 'find', 'every', 'competencies'],\n",
    " ['study', 'find', 'every', 'little', 'intelligence'],\n",
    " ['study', 'find', 'the', 'smart', 'etymology'],\n",
    " ['study', 'find', 'india'],\n",
    " ['study', 'find', 'some', 'applied', 'grammar'],\n",
    " ['study', 'find', 'a', 'western', 'term'],\n",
    " ['study', 'find', 'a', 'hard', 'intelligence'],\n",
    " ['study', 'find', 'linguistics', 'structured'],\n",
    " ['study', 'find', 'time', 'plato'],\n",
    " ['study', 'find', 'the', 'historical', 'history'],\n",
    " ['study', 'find', 'the', 'computational', 'intelligence']]\n",
    "[item for sublist in gen for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e0825c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Simple english Wikipedia\n",
    "wiki_wiki = wikipediaapi.Wikipedia('simple')\n",
    "# Find page on linguistics\n",
    "page_py = wiki_wiki.page('Linguistics')\n",
    "LingSimptxt = page_py.text\n",
    "# Find page on AI\n",
    "page_py = wiki_wiki.page('Artificial_intelligence')\n",
    "AISimptxt = page_py.text\n",
    "S = AISimptxt.split('\\n')\n",
    "i = S.index('Related pages')\n",
    "AISimpsentences = S[:i]\n",
    "S = LingSimptxt.split('\\n')\n",
    "i = S.index('== References ==')\n",
    "LingSimpsentences = S[:i]\n",
    "\n",
    "# Simplify and tokenize data\n",
    "aisimpsent, aisimptokens, aisimptoken = text_to_tokens(AISimpsentences)\n",
    "lingsimpsent, lingsimptokens, lingsimptoken = text_to_tokens(LingSimpsentences)\n",
    "aisent, aitokens, aitoken = text_to_tokens(AIsentences)\n",
    "lingsent, lingtokens, lingtoken = text_to_tokens(Lingsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe531123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentences:  69\n",
      "Average len of sentence:  19.565217391304348\n",
      "No of words:  1350\n",
      "No. of hapaxes:  370\n",
      "Size of Vocabulary:  529\n",
      "[('the', 69), ('and', 53), ('of', 51), ('in', 39), ('a', 34), ('to', 33), ('language', 29), ('study', 22), ('is', 20), ('linguistics', 19)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from nltk import tokenize, LegalitySyllableTokenizer\n",
    "from nltk.corpus import words\n",
    "import wikipediaapi\n",
    "import nltk\n",
    "\n",
    "def analyse(text):\n",
    "    \"\"\"\n",
    "    list of strings representing sentences\n",
    "    \"\"\"\n",
    "    sent, tokens, token = text_to_tokens(text)\n",
    "    \n",
    "    # N sentences\n",
    "    print(\"No of sentences: \", len(sent))\n",
    "    \n",
    "    # Length of sentences\n",
    "    print(\"Average len of sentence: \", sum([len(x) for x in tokens])/len(sent))\n",
    "    \n",
    "    # N of words\n",
    "    print(\"No of words: \", len(token))\n",
    "    \n",
    "    # Hapaxes\n",
    "    count = {}\n",
    "    for sentence in tokens:\n",
    "        for word in sentence:\n",
    "            if word in count:\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word] = 1\n",
    "\n",
    "    hapaxes = []\n",
    "    for word in count.keys():\n",
    "        if count[word] == 1:\n",
    "            hapaxes.append(word)\n",
    "    \n",
    "    print(\"No. of hapaxes: \",len(hapaxes))\n",
    "    \n",
    "    # Size of vocab \n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    lemmas = {wnl.lemmatize(t) for t in token}\n",
    "    print(\"Size of Vocabulary: \",len(lemmas))\n",
    "    \n",
    "    # Most frequent words\n",
    "    freq = dict()\n",
    "    for word in token:\n",
    "        w = wnl.lemmatize(word)\n",
    "        if w in freq:\n",
    "            freq[w] += 1\n",
    "        else:\n",
    "            freq[w] = 1\n",
    "    freq = list(freq.items())\n",
    "    freq.sort(reverse = True, key = lambda x: x[1])\n",
    "    print(freq[:10])\n",
    "\n",
    "    frequencies = [x[1] for x in freq]\n",
    "    lab = [x[0] for x in freq]\n",
    "    \n",
    "    \n",
    "    plt.bar(lab[:10],frequencies[:10])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "analyse(AISimpsentences + LingSimpsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(LingSimpsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32b77470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english wikipedia\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "# Find page on AI\n",
    "page_py = wiki_wiki.page('Artificial_intelligence')\n",
    "AItxt = page_py.text\n",
    "\n",
    "# Find page on linguistics\n",
    "page_py = wiki_wiki.page('Linguistics')\n",
    "Lingtxt = page_py.text\n",
    "\n",
    "# Save sentences\n",
    "S = AItxt.split('\\n')\n",
    "i = S.index('See also')\n",
    "AIsentences = S[:i]\n",
    "S = Lingtxt.split('\\n')\n",
    "i = S.index('See also')\n",
    "Lingsentences = S[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3e0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "907f1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Simple english Wikipedia\n",
    "wiki_wiki = wikipediaapi.Wikipedia('simple')\n",
    "# Find page on linguistics\n",
    "page_py = wiki_wiki.page('Linguistics')\n",
    "LingSimptxt = page_py.text\n",
    "# Find page on AI\n",
    "page_py = wiki_wiki.page('Artificial_intelligence')\n",
    "AISimptxt = page_py.text\n",
    "S = AISimptxt.split('\\n')\n",
    "i = S.index('Related pages')\n",
    "AISimpsentences = S[:i]\n",
    "S = LingSimptxt.split('\\n')\n",
    "i = S.index('== References ==')\n",
    "LingSimpsentences = S[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eabb8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify and tokenize data\n",
    "aisimpsent, aisimptokens, aisimptoken = text_to_tokens(AISimpsentences)\n",
    "lingsimpsent, lingsimptokens, lingsimptoken = text_to_tokens(LingSimpsentences)\n",
    "aisent, aitokens, aitoken = text_to_tokens(AIsentences)\n",
    "lingsent, lingtokens, lingtoken = text_to_tokens(Lingsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d208c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "grmmr = ''\n",
    "\n",
    "with open('tstgrmmr.txt') as f:\n",
    "    for line in f:\n",
    "        grmmr += line\n",
    "        \n",
    "grmmr = nltk.CFG.fromstring(grmmr)\n",
    "\n",
    "parser = nltk.parse.BottomUpChartParser(grmmr)\n",
    "\n",
    "parsable = ['linguistics is the study of language',\n",
    "           'people who study language are called linguists',\n",
    "            'many linguists compare languages to find similar properties',\n",
    "            'there are many ways to use linguistics every day',\n",
    "            'for many centuries most linguistic work was philology',\n",
    "            'literary theorists study the use of language in literature',\n",
    "            'discourse analysis is the study of entire conversations or texts',\n",
    "            'some linguists are applied linguists and use linguistics to do things',\n",
    "            'one part of historical linguistics is etymology the study of the history of words',\n",
    "            'the word etymology is first used to talk about the history behind a words meaning',\n",
    "            'semiotics for example is the general study of signs and symbols both within language and without',\n",
    "            'another part of linguistics is involved in understanding how languages are used in society or in the world',\n",
    "            'the part of linguistics that aims to find out how language works in the mind is known as psycholinguistics',\n",
    "            'John Mccarthy came up with the name artificial intelligence in 1955',\n",
    "            'in general use the term artificial intelligence means a programme which mimics human cognition',\n",
    "            'many approaches and tools have been tried',\n",
    "            'they work on their own without being encoded with commands',\n",
    "            'it is also a field of study which tries to make computers smart',\n",
    "            'humanized ai shows characteristics of all types of competencies',\n",
    "            'ai research really started with a conference at Dartmouth College in 1956',\n",
    "            'unfortunately researchers really underestimated just how hard some problems were',\n",
    "            'searching databases and doing calculations are things computers do better than people',\n",
    "            'it was a monthlong brainstorming session attended by many people with interests in ai',\n",
    "            'sociolinguistics studies how language is used in society',\n",
    "            'historical linguistics studies how languages change over time and how languages were in the past',\n",
    "            'forensic linguistics is used in crime investigations and computational linguistics is used to help make computers understand languages',\n",
    "            'linguists who study how languages are structured and how they work are said to study theoretical linguistics',\n",
    "            'the study of language began in india with panini the 5th century bc grammarian who wrote about the 3959 rules of sanskrit grammar',\n",
    "            'plato was the first western philosopher to write about semantics',\n",
    "            'some people also consider ai a danger to humanity if it continues to progress at its current pace'\n",
    "           ]\n",
    "for sentence in parsable:\n",
    "    parsed = False\n",
    "    \n",
    "    for i in parser.parse(sentence.split()):\n",
    "        parsed = True\n",
    "        break\n",
    "    \n",
    "    if not parsed:    \n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b96d339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "S= 'some people also consider ai a danger to humanity if it continues to progress at its current pace'.split()\n",
    "words = set()\n",
    "with open('tstgrmmr.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if re.findall('(\\'.*\\')', line):\n",
    "            words.add(re.findall('(\\'.*\\')', line)[0].strip('\\''))\n",
    "            \n",
    "with open('tstgrmmr.txt', 'a') as f:\n",
    "    for w in S:\n",
    "        if w not in words:\n",
    "            f.write(f' -> \\'{w}\\'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cce636",
   "metadata": {},
   "outputs": [],
   "source": [
    "LingSimpsentences.sort(key = lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in parser.parse( 'another part of linguistics is involved in understanding how languages are used in society or in the world'.split()):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef40b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['study', 'find', 'use'],\n",
       " ['study', 'find', 'discourse'],\n",
       " ['study', 'find', 'analysis'],\n",
       " ['study', 'find', 'linguists', 'society'],\n",
       " ['study', 'find', 'part', 'linguists'],\n",
       " ['study', 'find', 'crime'],\n",
       " ['study', 'find', 'part', 'research'],\n",
       " ['study', 'find', 'word', 'world'],\n",
       " ['study', 'find', 'use', 'session'],\n",
       " ['study', 'find', 'study', 'society'],\n",
       " ['study', 'find', 'language', 'session'],\n",
       " ['study', 'find', 'society', 'investigations'],\n",
       " ['study', 'find', 'semiotics', 'study'],\n",
       " ['study', 'find', 'language', 'structured'],\n",
       " ['study', 'find', 'discourse', 'plato'],\n",
       " ['study', 'find', 'history', 'meaning'],\n",
       " ['study', 'find', 'past'],\n",
       " ['study', 'find', 'part', 'name'],\n",
       " ['study', 'find', 'analysis', 'society'],\n",
       " ['study', 'find', 'semiotics', 'programme'],\n",
       " ['study', 'find', 'day', 'plato'],\n",
       " ['study', 'find', 'semiotics', 'grammar'],\n",
       " ['study', 'find', 'linguists', 'india'],\n",
       " ['study', 'find', 'etymology', 'grammarian'],\n",
       " ['study', 'find', 'part', 'plato'],\n",
       " ['study', 'find', 'literature', 'intelligence'],\n",
       " ['study', 'find', 'discourse', 'philosopher'],\n",
       " ['study', 'find', 'use', 'psycholinguistics'],\n",
       " ['study', 'find', 'word', 'past'],\n",
       " ['study', 'find', 'semiotics', 'semantics'],\n",
       " ['study', 'find', 'word', 'semiotics'],\n",
       " ['study', 'find', 'part', 'psycholinguistics'],\n",
       " ['study', 'find', 'world', 'name'],\n",
       " ['study', 'find', 'linguists', 'intelligence'],\n",
       " ['study', 'find', 'word', 'plato'],\n",
       " ['study', 'find', 'study', 'century'],\n",
       " ['study', 'find', 'society', 'literature'],\n",
       " ['study', 'find', 'linguistics', 'mind'],\n",
       " ['study', 'find', 'linguistics', 'linguists'],\n",
       " ['study', 'find', 'language', 'panini'],\n",
       " ['study', 'find', 'etymology', 'analysis'],\n",
       " ['study', 'find', 'etymology', 'grammar'],\n",
       " ['study', 'find', 'work', 'session'],\n",
       " ['study', 'find', 'example', 'word'],\n",
       " ['study', 'find', 'language', 'term'],\n",
       " ['study', 'find', 'semiotics', 'name'],\n",
       " ['study', 'find', 'day', 'structured'],\n",
       " ['study', 'find', 'study', 'linguistics'],\n",
       " ['study', 'find', 'day', 'funding'],\n",
       " ['study', 'find', 'history', 'field']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.generate import generate\n",
    "import random\n",
    "sentences = []\n",
    "for sentence in generate(grmmr, n=1000, depth = 10):\n",
    "    sentences.append(sentence)\n",
    "random.choices(sentences, k = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64b7388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = False\n",
    "grammr = []\n",
    "with open('tstgrmmr.txt') as f:\n",
    "    for rule in f:\n",
    "        if re.findall('(\\w*) -', rule) != a:\n",
    "            a = re.findall('(\\w*) -', rule)\n",
    "            grammr.append([rule])\n",
    "        else:\n",
    "            grammr[-1].append(rule)\n",
    "        \n",
    "        \n",
    "shuffled = []\n",
    "for i in grammr:\n",
    "    random.shuffle(i)\n",
    "    shuffled.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1754a584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADVP']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('(\\w*) -', 'ADVP -> RB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aaec5088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['S -> PP NP VP\\n',\n",
       "  'S -> NP VP\\n',\n",
       "  'S -> VP\\n',\n",
       "  'S -> S CC S\\n',\n",
       "  'S -> NP ADJP\\n',\n",
       "  'S -> ADVP NP ADVP VP\\n',\n",
       "  'S -> S PRN NP VP\\n',\n",
       "  'S -> NP ADVP VP\\n'],\n",
       " ['VP -> VBN PP\\n',\n",
       "  'VP -> VBZ PP\\n',\n",
       "  'VP -> VB PP\\n',\n",
       "  'VP -> VBN NP\\n',\n",
       "  'VP -> VBD PRT PP PP\\n',\n",
       "  'VP -> VBP NP S\\n',\n",
       "  'VP -> VBZ S\\n',\n",
       "  'VP -> VBD PP NP\\n',\n",
       "  'VP -> VBP NP NP SBAR\\n',\n",
       "  'VP -> VBG NP SBAR\\n',\n",
       "  'VP -> VBZ SBAR\\n',\n",
       "  'VP -> TO VP\\n',\n",
       "  'VP -> VBP\\n',\n",
       "  'VP -> VBD PP PP\\n',\n",
       "  'VP -> VB S\\n',\n",
       "  'VP -> VBG VP\\n',\n",
       "  'VP -> VBG SBAR\\n',\n",
       "  'VP -> VBD SBAR\\n',\n",
       "  'VP -> VBG NP\\n',\n",
       "  'VP -> VBP PP\\n',\n",
       "  'VP -> VBZ NP\\n',\n",
       "  'VP -> VBD NP\\n',\n",
       "  'VP -> VBP PP PP\\n',\n",
       "  'VP -> VP CC VP\\n',\n",
       "  'VP -> VBZ NP PP\\n',\n",
       "  'VP -> VBP VP\\n',\n",
       "  'VP -> VBP SBAR\\n',\n",
       "  'VP -> VBZ VP\\n',\n",
       "  'VP -> VBZ ADVP NP\\n',\n",
       "  'VP -> VBN VP\\n',\n",
       "  'VP -> VBG PP\\n',\n",
       "  'VP -> VBZ ADVP VP\\n',\n",
       "  'VP -> VBP ADJP\\n',\n",
       "  'VP -> VB NP NP\\n',\n",
       "  'VP -> VBN\\n',\n",
       "  'VP -> VBD VP\\n',\n",
       "  'VP -> VBD\\n',\n",
       "  'VP -> VB NP\\n',\n",
       "  'VP -> VBN PP PP\\n',\n",
       "  'VP -> VB PRT SBAR\\n',\n",
       "  'VP -> VBP NP PP\\n',\n",
       "  'VP -> VBD ADJP\\n',\n",
       "  'VP -> VBN S\\n',\n",
       "  'VP -> VBP NP\\n',\n",
       "  'VP -> VBD PP\\n'],\n",
       " ['\\n'],\n",
       " ['NP -> CD NN\\n',\n",
       "  'NP -> NP SBAR\\n',\n",
       "  'NP -> JJ NNSF\\n',\n",
       "  'NP -> RBS JJ NN\\n',\n",
       "  'NP -> EX\\n',\n",
       "  'NP -> NNS\\n',\n",
       "  'NP -> NP VP\\n',\n",
       "  'NP -> DT CD NNS\\n',\n",
       "  'NP -> DT NN\\n',\n",
       "  'NP -> JJ NNS S\\n',\n",
       "  'NP -> NNP\\n',\n",
       "  'NP -> DT NNS\\n',\n",
       "  'NP -> NP SBAR\\n',\n",
       "  'NP -> NP CC NP\\n',\n",
       "  'NP -> NP NP\\n',\n",
       "  'NP -> JJ NN\\n',\n",
       "  'NP -> PRP\\n',\n",
       "  'NP -> NNP NNP\\n',\n",
       "  'NP -> DT JJ JJ NN S\\n',\n",
       "  'NP -> DT ADJP NN NN\\n',\n",
       "  'NP -> DT NML NN NN\\n',\n",
       "  'NP -> JJ NNS\\n',\n",
       "  'NP -> DT NNS NN\\n',\n",
       "  'NP -> DT NN JJ NN\\n',\n",
       "  'NP -> DT JJ NN\\n',\n",
       "  'NP -> NNP NN\\n',\n",
       "  'NP -> NN\\n',\n",
       "  'NP -> CD\\n',\n",
       "  'NP -> PRP JJ\\n',\n",
       "  'NP -> NN NN\\n',\n",
       "  'NP -> JJ NP CC NP\\n',\n",
       "  'NP -> PRP JJ NN\\n',\n",
       "  'NP -> NP PP\\n'],\n",
       " ['\\n'],\n",
       " ['SBAR -> WHNP S\\n',\n",
       "  'SBAR -> WHADJP S\\n',\n",
       "  'SBAR -> IN S\\n',\n",
       "  'SBAR -> WHADVP S\\n'],\n",
       " ['\\n'],\n",
       " ['WHNP -> WP\\n'],\n",
       " ['\\n'],\n",
       " ['PP -> DT PP CC PP\\n',\n",
       "  'PP -> IN NP\\n',\n",
       "  'PP -> IN S\\n',\n",
       "  'PP -> PP CC PP\\n',\n",
       "  'PP -> IN\\n'],\n",
       " ['ADJP -> ADJP PP\\n', 'ADJP -> JJR\\n', 'ADJP -> JJ\\n'],\n",
       " ['ADVP -> RB\\n'],\n",
       " ['SBAR -> WHNP S\\n', 'SBAR -> SBAR CC SBAR\\n', 'SBAR -> WHADVP S\\n'],\n",
       " ['WHNP -> WDT\\n'],\n",
       " ['WHADVP -> WRB\\n'],\n",
       " ['WHADJP -> RB WRB JJ\\n'],\n",
       " ['PRT -> RP\\n'],\n",
       " ['PRN -> SINV\\n'],\n",
       " ['SINV -> VP NP\\n'],\n",
       " ['\\n'],\n",
       " [\"NN -> 'linguistics'\\n\"],\n",
       " [\"NNS -> 'linguistics'\\n\"],\n",
       " [\"VBZ -> 'is'\\n\"],\n",
       " [\"DT -> 'the'\\n\"],\n",
       " [\"NN -> 'study'\\n\"],\n",
       " [\"IN -> 'of'\\n\"],\n",
       " [\"NN -> 'language'\\n\"],\n",
       " [\"NNS -> 'people'\\n\"],\n",
       " [\"WP -> 'who'\\n\"],\n",
       " [\"VBP -> 'study'\\n\", \"VBP -> 'are'\\n\"],\n",
       " [\"VBN -> 'called'\\n\"],\n",
       " [\"NN -> 'linguists'\\n\"],\n",
       " [\"JJ -> 'many'\\n\"],\n",
       " [\"VBP -> 'compare'\\n\"],\n",
       " [\"NNS -> 'languages'\\n\"],\n",
       " [\"TO -> 'to'\\n\"],\n",
       " [\"VB -> 'find'\\n\"],\n",
       " [\"JJ -> 'similar'\\n\"],\n",
       " [\"NNS -> 'properties'\\n\"],\n",
       " [\"EX -> 'there'\\n\"],\n",
       " [\"VBP -> 'are'\\n\"],\n",
       " [\"JJ -> 'many'\\n\"],\n",
       " [\"NNS -> 'ways'\\n\"],\n",
       " [\"VB -> 'use'\\n\"],\n",
       " [\"DT -> 'every'\\n\"],\n",
       " [\"NN -> 'day'\\n\"],\n",
       " [\"IN -> 'for'\\n\"],\n",
       " [\"JJ -> 'many'\\n\"],\n",
       " [\"NNS -> 'centuries'\\n\"],\n",
       " [\"RBS -> 'most'\\n\"],\n",
       " [\"JJ -> 'linguistic'\\n\"],\n",
       " [\"NN -> 'work'\\n\"],\n",
       " [\"VBD -> 'was'\\n\"],\n",
       " [\"JJ -> 'literary'\\n\", \"JJ -> 'philology'\\n\"],\n",
       " [\"NNS -> 'theorists'\\n\"],\n",
       " [\"VBP -> 'study'\\n\"],\n",
       " [\"NN -> 'use'\\n\"],\n",
       " [\"IN -> 'in'\\n\", \"IN -> 'of'\\n\"],\n",
       " [\"NN -> 'analysis'\\n\", \"NN -> 'discourse'\\n\", \"NN -> 'literature'\\n\"],\n",
       " [\"JJ -> 'entire'\\n\"],\n",
       " [\"NNS -> 'conversations'\\n\"],\n",
       " [\"CC -> 'or'\\n\"],\n",
       " [\"NNS -> 'texts'\\n\"],\n",
       " [\"DT -> 'some'\\n\"],\n",
       " [\"JJ -> 'applied'\\n\"],\n",
       " [\"CC -> 'and'\\n\"],\n",
       " [\"VBP -> 'use'\\n\"],\n",
       " [\"VB -> 'do'\\n\"],\n",
       " [\"VBP -> 'do'\\n\"],\n",
       " [\"NNS -> 'things'\\n\"],\n",
       " [\"CD -> 'one'\\n\"],\n",
       " [\"NN -> 'part'\\n\"],\n",
       " [\"IN -> 'of'\\n\"],\n",
       " [\"JJ -> 'historical'\\n\"],\n",
       " [\"NN -> 'history'\\n\"],\n",
       " [\"NNS -> 'words'\\n\"],\n",
       " [\"RB -> 'etymology'\\n\"],\n",
       " [\"NN -> 'etymology' \\n\"],\n",
       " [\"RB -> 'first'\\n\"],\n",
       " [\"JJ -> 'first'\\n\"],\n",
       " [\"VBN -> 'used'\\n\"],\n",
       " [\"VB -> 'talk'\\n\"],\n",
       " [\"IN -> 'behind'\\n\", \"IN -> 'about'\\n\"],\n",
       " [\"DT -> 'a'\\n\"],\n",
       " [\"NN -> 'semiotics'\\n\",\n",
       "  \"NN -> 'example'\\n\",\n",
       "  \"NN -> 'word' \\n\",\n",
       "  \"NN -> 'meaning'\\n\"],\n",
       " [\"JJ -> 'general'\\n\"],\n",
       " [\"NNS -> 'symbols'\\n\", \"NNS -> 'signs'\\n\"],\n",
       " [\"DT -> 'both'\\n\"],\n",
       " [\"IN -> 'within'\\n\", \"IN -> 'without'\\n\"],\n",
       " [\"DT -> 'another'\\n\"],\n",
       " [\"VBN -> 'involved'\\n\"],\n",
       " [\"VBG -> 'understanding'\\n\"],\n",
       " [\"NNS -> 'languages'\\n\"],\n",
       " [\"NN -> 'world'\\n\", \"NN -> 'society'\\n\"],\n",
       " [\"WDT -> 'that'\\n\"],\n",
       " [\"VBZ -> 'aims'\\n\"],\n",
       " [\"RP -> 'out'\\n\"],\n",
       " [\"VBZ -> 'works'\\n\"],\n",
       " [\"NN -> 'mind'\\n\"],\n",
       " [\"VBN -> 'known'\\n\"],\n",
       " [\"IN -> 'as'\\n\"],\n",
       " [\"NN -> 'psycholinguistics'\\n\"],\n",
       " [\"WRB -> 'how'\\n\"],\n",
       " [\"NNP -> 'John'\\n\", \"NNP -> 'Mccarthy'\\n\"],\n",
       " [\"VBD -> 'came'\\n\"],\n",
       " [\"RP -> 'up'\\n\"],\n",
       " [\"IN -> 'with'\\n\"],\n",
       " [\"NN -> 'name'\\n\"],\n",
       " [\"JJ -> 'artificial'\\n\"],\n",
       " [\"NN -> 'intelligence'\\n\"],\n",
       " [\"CD -> '1955'\\n\"],\n",
       " [\"NN -> 'term'\\n\"],\n",
       " [\"VBZ -> 'means'\\n\"],\n",
       " [\"NN -> 'programme'\\n\"],\n",
       " [\"WDT -> 'which'\\n\"],\n",
       " [\"VBZ -> 'mimics'\\n\"],\n",
       " [\"JJ -> 'human'\\n\"],\n",
       " [\"NN -> 'cognition'\\n\"],\n",
       " [\"NNS -> 'approaches'\\n\", \"NNS -> 'tools'\\n\"],\n",
       " [\"VBP -> 'have'\\n\"],\n",
       " [\"VBN -> 'been'\\n\", \"VBN -> 'tried'\\n\"],\n",
       " [\"PRP -> 'they'\\n\"],\n",
       " [\"IN -> 'on'\\n\"],\n",
       " [\"PRP -> 'their'\\n\"],\n",
       " [\"JJ -> 'own'\\n\"],\n",
       " [\"VBG -> 'being'\\n\"],\n",
       " [\"VBN -> 'encoded'\\n\"],\n",
       " [\"NNS -> 'commands'\\n\"],\n",
       " [\"PRP -> 'it'\\n\"],\n",
       " [\"RB -> 'also'\\n\"],\n",
       " [\"NN -> 'field'\\n\"],\n",
       " [\"VBZ -> 'tries'\\n\"],\n",
       " [\"VB -> 'make'\\n\"],\n",
       " [\"NNS -> 'computers'\\n\"],\n",
       " [\"JJ -> 'smart'\\n\"],\n",
       " [\"NNP -> 'humanized'\\n\", \"NNP -> 'ai'\\n\"],\n",
       " [\"VBZ -> 'shows'\\n\"],\n",
       " [\"NNS -> 'characteristics'\\n\"],\n",
       " [\"DT -> 'all'\\n\"],\n",
       " [\"NNS -> 'types'\\n\", \"NNS -> 'competencies'\\n\"],\n",
       " [\"NN -> 'research'\\n\"],\n",
       " [\"RB -> 'really'\\n\"],\n",
       " [\"VBD -> 'started'\\n\"],\n",
       " [\"NN -> 'conference'\\n\"],\n",
       " [\"IN -> 'at'\\n\"],\n",
       " [\"NNP -> 'College'\\n\", \"NNP -> 'Dartmouth'\\n\"],\n",
       " [\"CD -> '1956'\\n\"],\n",
       " [\"RB -> 'unfortunately'\\n\"],\n",
       " [\"NNS -> 'researchers'\\n\"],\n",
       " [\"VBD -> 'underestimated'\\n\"],\n",
       " [\"RB -> 'just'\\n\"],\n",
       " [\"JJ -> 'hard'\\n\"],\n",
       " [\"NNS -> 'problems'\\n\"],\n",
       " [\"VBD -> 'were'\\n\"],\n",
       " [\"NN -> 'funding'\\n\"],\n",
       " [\"VBN -> 'cut'\\n\"],\n",
       " [\"VBG -> 'starting'\\n\"],\n",
       " [\"DT -> 'an'\\n\"],\n",
       " [\"NN -> 'winter'\\n\"],\n",
       " [\"WRB -> 'where'\\n\"],\n",
       " [\"JJ -> 'little'\\n\"],\n",
       " [\"VBN -> 'done'\\n\"],\n",
       " [\"VBG -> 'searching'\\n\"],\n",
       " [\"NNS -> 'databases'\\n\"],\n",
       " [\"VBG -> 'doing'\\n\"],\n",
       " [\"NNS -> 'calculations'\\n\"],\n",
       " [\"JJR -> 'better'\\n\"],\n",
       " [\"IN -> 'than'\\n\"],\n",
       " [\"JJ -> 'monthlong'\\n\"],\n",
       " [\"NN -> 'session'\\n\", \"NN -> 'brainstorming'\\n\"],\n",
       " [\"VBN -> 'attended'\\n\"],\n",
       " [\"IN -> 'by'\\n\"],\n",
       " [\"NNS -> 'interests'\\n\"],\n",
       " [\"VBZ -> 'studies'\\n\"],\n",
       " [\"NN -> 'sociolinguistics'\\n\"],\n",
       " [\"VBP -> 'change'\\n\"],\n",
       " [\"IN -> 'over'\\n\"],\n",
       " [\"NN -> 'time'\\n\", \"NN -> 'past'\\n\"],\n",
       " [\"JJ -> 'forensic'\\n\"],\n",
       " [\"NN -> 'crime'\\n\", \"NN -> 'investigations'\\n\"],\n",
       " [\"JJ -> 'computational'\\n\"],\n",
       " [\"VB -> 'help'\\n\", \"VB -> 'understand'\\n\"],\n",
       " [\"NN -> 'structured'\\n\"],\n",
       " [\"VBN -> 'said'\\n\"],\n",
       " [\"JJ -> 'theoretical'\\n\"],\n",
       " [\"VBD -> 'began'\\n\"],\n",
       " [\"NN -> 'panini'\\n\", \"NN -> 'india'\\n\"],\n",
       " [\"JJ -> '5th'\\n\"],\n",
       " [\"NN -> 'bc'\\n\", \"NN -> 'century'\\n\", \"NN -> 'grammarian'\\n\"],\n",
       " [\"VBD -> 'wrote'\\n\"],\n",
       " [\"CD -> '3959'\\n\"],\n",
       " [\"NNS -> 'rules'\\n\"],\n",
       " [\"JJ -> 'sanskrit'\\n\"],\n",
       " [\"NN -> 'plato'\\n\", \"NN -> 'grammar'\\n\"],\n",
       " [\"JJ -> 'western'\\n\"],\n",
       " [\"NN -> 'philosopher'\\n\"],\n",
       " [\"VB -> 'write'\\n\"],\n",
       " [\"NN -> 'semantics'\\n\"],\n",
       " [\"VBP -> 'consider'\\n\"],\n",
       " [\"NN -> 'danger'\\n\", \"NN -> 'humanity'\\n\"],\n",
       " [\"IN -> 'if'\\n\"],\n",
       " [\"VBZ -> 'continues'\\n\"],\n",
       " [\"VB -> 'progress'\\n\"],\n",
       " [\"PRP -> 'its'\\n\"],\n",
       " [\"JJ -> 'current'\\n\"],\n",
       " [\"NN -> 'pace'\\n\"],\n",
       " [\"IN -> 'to'\\n\"]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5614fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammrstr = ''\n",
    "for i in shuffled:\n",
    "    grammrstr += ''.join(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61297a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"S -> PP NP VP\\nS -> NP VP\\nS -> NP ADVP VP\\nS -> S CC S\\nS -> VP\\nS -> NP ADJP\\nS -> S PRN NP VP\\nS -> ADVP NP ADVP VP\\nVP -> VBG PP\\nVP -> VBG NP SBAR\\nVP -> VBN S\\nVP -> VBP PP PP\\nVP -> VBD PP\\nVP -> VBP NP PP\\nVP -> VBD NP\\nVP -> VBZ NP\\nVP -> VBP PP\\nVP -> VBP NP\\nVP -> VBD PP PP\\nVP -> VBP SBAR\\nVP -> VBD PP NP\\nVP -> VBP VP\\nVP -> VBN VP\\nVP -> VBD SBAR\\nVP -> VB PRT SBAR\\nVP -> VBN PP PP\\nVP -> VBN\\nVP -> VB S\\nVP -> VBN NP\\nVP -> VBD VP\\nVP -> VBZ SBAR\\nVP -> TO VP\\nVP -> VBZ S\\nVP -> VB NP NP\\nVP -> VBD PRT PP PP\\nVP -> VBD ADJP\\nVP -> VB NP\\nVP -> VBG VP\\nVP -> VBN PP\\nVP -> VB PP\\nVP -> VBG NP\\nVP -> VP CC VP\\nVP -> VBP NP S\\nVP -> VBZ ADVP NP\\nVP -> VBP NP NP SBAR\\nVP -> VBZ PP\\nVP -> VBP ADJP\\nVP -> VBZ NP PP\\nVP -> VBZ ADVP VP\\nVP -> VBZ VP\\nVP -> VBD\\nVP -> VBP\\nVP -> VBG SBAR\\n\\nNP -> JJ NN\\nNP -> PRP\\nNP -> DT JJ JJ NN S\\nNP -> EX\\nNP -> PRP JJ NN\\nNP -> JJ NP CC NP\\nNP -> NP VP\\nNP -> NP CC NP\\nNP -> JJ NNS S\\nNP -> NN\\nNP -> RBS JJ NN\\nNP -> DT JJ NN\\nNP -> NP SBAR\\nNP -> NN NN\\nNP -> NNP\\nNP -> PRP JJ\\nNP -> NP NP\\nNP -> NNP NNP\\nNP -> NNP NN\\nNP -> JJ NNS\\nNP -> NP SBAR\\nNP -> DT CD NNS\\nNP -> CD NN\\nNP -> DT ADJP NN NN\\nNP -> DT NML NN NN\\nNP -> DT NN\\nNP -> NNS\\nNP -> NP PP\\nNP -> JJ NNSF\\nNP -> DT NNS\\nNP -> DT NNS NN\\nNP -> DT NN JJ NN\\nNP -> CD\\n\\nSBAR -> WHADJP S\\nSBAR -> WHADVP S\\nSBAR -> IN S\\nSBAR -> WHNP S\\n\\nWHNP -> WP\\n\\nPP -> IN\\nPP -> IN NP\\nPP -> IN S\\nPP -> PP CC PP\\nPP -> DT PP CC PP\\nADJP -> JJ\\nADJP -> ADJP PP\\nADJP -> JJR\\nADVP -> RB\\nSBAR -> SBAR CC SBAR\\nSBAR -> WHNP S\\nSBAR -> WHADVP S\\nWHNP -> WDT\\nWHADVP -> WRB\\nWHADJP -> RB WRB JJ\\nPRT -> RP\\nPRN -> SINV\\nSINV -> VP NP\\n\\nNN -> 'linguistics'\\nNNS -> 'linguistics'\\nVBZ -> 'is'\\nDT -> 'the'\\nNN -> 'study'\\nIN -> 'of'\\nNN -> 'language'\\nNNS -> 'people'\\nWP -> 'who'\\nVBP -> 'are'\\nVBP -> 'study'\\nVBN -> 'called'\\nNN -> 'linguists'\\nJJ -> 'many'\\nVBP -> 'compare'\\nNNS -> 'languages'\\nTO -> 'to'\\nVB -> 'find'\\nJJ -> 'similar'\\nNNS -> 'properties'\\nEX -> 'there'\\nVBP -> 'are'\\nJJ -> 'many'\\nNNS -> 'ways'\\nVB -> 'use'\\nDT -> 'every'\\nNN -> 'day'\\nIN -> 'for'\\nJJ -> 'many'\\nNNS -> 'centuries'\\nRBS -> 'most'\\nJJ -> 'linguistic'\\nNN -> 'work'\\nVBD -> 'was'\\nJJ -> 'literary'\\nJJ -> 'philology'\\nNNS -> 'theorists'\\nVBP -> 'study'\\nNN -> 'use'\\nIN -> 'of'\\nIN -> 'in'\\nNN -> 'literature'\\nNN -> 'analysis'\\nNN -> 'discourse'\\nJJ -> 'entire'\\nNNS -> 'conversations'\\nCC -> 'or'\\nNNS -> 'texts'\\nDT -> 'some'\\nJJ -> 'applied'\\nCC -> 'and'\\nVBP -> 'use'\\nVB -> 'do'\\nVBP -> 'do'\\nNNS -> 'things'\\nCD -> 'one'\\nNN -> 'part'\\nIN -> 'of'\\nJJ -> 'historical'\\nNN -> 'history'\\nNNS -> 'words'\\nRB -> 'etymology'\\nNN -> 'etymology' \\nRB -> 'first'\\nJJ -> 'first'\\nVBN -> 'used'\\nVB -> 'talk'\\nIN -> 'behind'\\nIN -> 'about'\\nDT -> 'a'\\nNN -> 'example'\\nNN -> 'word' \\nNN -> 'meaning'\\nNN -> 'semiotics'\\nJJ -> 'general'\\nNNS -> 'signs'\\nNNS -> 'symbols'\\nDT -> 'both'\\nIN -> 'within'\\nIN -> 'without'\\nDT -> 'another'\\nVBN -> 'involved'\\nVBG -> 'understanding'\\nNNS -> 'languages'\\nNN -> 'society'\\nNN -> 'world'\\nWDT -> 'that'\\nVBZ -> 'aims'\\nRP -> 'out'\\nVBZ -> 'works'\\nNN -> 'mind'\\nVBN -> 'known'\\nIN -> 'as'\\nNN -> 'psycholinguistics'\\nWRB -> 'how'\\nNNP -> 'Mccarthy'\\nNNP -> 'John'\\nVBD -> 'came'\\nRP -> 'up'\\nIN -> 'with'\\nNN -> 'name'\\nJJ -> 'artificial'\\nNN -> 'intelligence'\\nCD -> '1955'\\nNN -> 'term'\\nVBZ -> 'means'\\nNN -> 'programme'\\nWDT -> 'which'\\nVBZ -> 'mimics'\\nJJ -> 'human'\\nNN -> 'cognition'\\nNNS -> 'tools'\\nNNS -> 'approaches'\\nVBP -> 'have'\\nVBN -> 'been'\\nVBN -> 'tried'\\nPRP -> 'they'\\nIN -> 'on'\\nPRP -> 'their'\\nJJ -> 'own'\\nVBG -> 'being'\\nVBN -> 'encoded'\\nNNS -> 'commands'\\nPRP -> 'it'\\nRB -> 'also'\\nNN -> 'field'\\nVBZ -> 'tries'\\nVB -> 'make'\\nNNS -> 'computers'\\nJJ -> 'smart'\\nNNP -> 'ai'\\nNNP -> 'humanized'\\nVBZ -> 'shows'\\nNNS -> 'characteristics'\\nDT -> 'all'\\nNNS -> 'types'\\nNNS -> 'competencies'\\nNN -> 'research'\\nRB -> 'really'\\nVBD -> 'started'\\nNN -> 'conference'\\nIN -> 'at'\\nNNP -> 'Dartmouth'\\nNNP -> 'College'\\nCD -> '1956'\\nRB -> 'unfortunately'\\nNNS -> 'researchers'\\nVBD -> 'underestimated'\\nRB -> 'just'\\nJJ -> 'hard'\\nNNS -> 'problems'\\nVBD -> 'were'\\nNN -> 'funding'\\nVBN -> 'cut'\\nVBG -> 'starting'\\nDT -> 'an'\\nNN -> 'winter'\\nWRB -> 'where'\\nJJ -> 'little'\\nVBN -> 'done'\\nVBG -> 'searching'\\nNNS -> 'databases'\\nVBG -> 'doing'\\nNNS -> 'calculations'\\nJJR -> 'better'\\nIN -> 'than'\\nJJ -> 'monthlong'\\nNN -> 'brainstorming'\\nNN -> 'session'\\nVBN -> 'attended'\\nIN -> 'by'\\nNNS -> 'interests'\\nVBZ -> 'studies'\\nNN -> 'sociolinguistics'\\nVBP -> 'change'\\nIN -> 'over'\\nNN -> 'past'\\nNN -> 'time'\\nJJ -> 'forensic'\\nNN -> 'investigations'\\nNN -> 'crime'\\nJJ -> 'computational'\\nVB -> 'help'\\nVB -> 'understand'\\nNN -> 'structured'\\nVBN -> 'said'\\nJJ -> 'theoretical'\\nVBD -> 'began'\\nNN -> 'india'\\nNN -> 'panini'\\nJJ -> '5th'\\nNN -> 'bc'\\nNN -> 'century'\\nNN -> 'grammarian'\\nVBD -> 'wrote'\\nCD -> '3959'\\nNNS -> 'rules'\\nJJ -> 'sanskrit'\\nNN -> 'plato'\\nNN -> 'grammar'\\nJJ -> 'western'\\nNN -> 'philosopher'\\nVB -> 'write'\\nNN -> 'semantics'\\nVBP -> 'consider'\\nNN -> 'danger'\\nNN -> 'humanity'\\nIN -> 'if'\\nVBZ -> 'continues'\\nVB -> 'progress'\\nPRP -> 'its'\\nJJ -> 'current'\\nNN -> 'pace'\\nIN -> 'to'\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammrstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ce03337",
   "metadata": {},
   "outputs": [],
   "source": [
    "grmmr = nltk.CFG.fromstring(grammrstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9dd6ff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 323 productions (start state = S)\n",
      "    S -> PP NP VP\n",
      "    S -> NP VP\n",
      "    S -> NP ADVP VP\n",
      "    S -> S CC S\n",
      "    S -> VP\n",
      "    S -> NP ADJP\n",
      "    S -> S PRN NP VP\n",
      "    S -> ADVP NP ADVP VP\n",
      "    VP -> VBG PP\n",
      "    VP -> VBG NP SBAR\n",
      "    VP -> VBN S\n",
      "    VP -> VBP PP PP\n",
      "    VP -> VBD PP\n",
      "    VP -> VBP NP PP\n",
      "    VP -> VBD NP\n",
      "    VP -> VBZ NP\n",
      "    VP -> VBP PP\n",
      "    VP -> VBP NP\n",
      "    VP -> VBD PP PP\n",
      "    VP -> VBP SBAR\n",
      "    VP -> VBD PP NP\n",
      "    VP -> VBP VP\n",
      "    VP -> VBN VP\n",
      "    VP -> VBD SBAR\n",
      "    VP -> VB PRT SBAR\n",
      "    VP -> VBN PP PP\n",
      "    VP -> VBN\n",
      "    VP -> VB S\n",
      "    VP -> VBN NP\n",
      "    VP -> VBD VP\n",
      "    VP -> VBZ SBAR\n",
      "    VP -> TO VP\n",
      "    VP -> VBZ S\n",
      "    VP -> VB NP NP\n",
      "    VP -> VBD PRT PP PP\n",
      "    VP -> VBD ADJP\n",
      "    VP -> VB NP\n",
      "    VP -> VBG VP\n",
      "    VP -> VBN PP\n",
      "    VP -> VB PP\n",
      "    VP -> VBG NP\n",
      "    VP -> VP CC VP\n",
      "    VP -> VBP NP S\n",
      "    VP -> VBZ ADVP NP\n",
      "    VP -> VBP NP NP SBAR\n",
      "    VP -> VBZ PP\n",
      "    VP -> VBP ADJP\n",
      "    VP -> VBZ NP PP\n",
      "    VP -> VBZ ADVP VP\n",
      "    VP -> VBZ VP\n",
      "    VP -> VBD\n",
      "    VP -> VBP\n",
      "    VP -> VBG SBAR\n",
      "    NP -> JJ NN\n",
      "    NP -> PRP\n",
      "    NP -> DT JJ JJ NN S\n",
      "    NP -> EX\n",
      "    NP -> PRP JJ NN\n",
      "    NP -> JJ NP CC NP\n",
      "    NP -> NP VP\n",
      "    NP -> NP CC NP\n",
      "    NP -> JJ NNS S\n",
      "    NP -> NN\n",
      "    NP -> RBS JJ NN\n",
      "    NP -> DT JJ NN\n",
      "    NP -> NP SBAR\n",
      "    NP -> NN NN\n",
      "    NP -> NNP\n",
      "    NP -> PRP JJ\n",
      "    NP -> NP NP\n",
      "    NP -> NNP NNP\n",
      "    NP -> NNP NN\n",
      "    NP -> JJ NNS\n",
      "    NP -> NP SBAR\n",
      "    NP -> DT CD NNS\n",
      "    NP -> CD NN\n",
      "    NP -> DT ADJP NN NN\n",
      "    NP -> DT NML NN NN\n",
      "    NP -> DT NN\n",
      "    NP -> NNS\n",
      "    NP -> NP PP\n",
      "    NP -> JJ NNSF\n",
      "    NP -> DT NNS\n",
      "    NP -> DT NNS NN\n",
      "    NP -> DT NN JJ NN\n",
      "    NP -> CD\n",
      "    SBAR -> WHADJP S\n",
      "    SBAR -> WHADVP S\n",
      "    SBAR -> IN S\n",
      "    SBAR -> WHNP S\n",
      "    WHNP -> WP\n",
      "    PP -> IN\n",
      "    PP -> IN NP\n",
      "    PP -> IN S\n",
      "    PP -> PP CC PP\n",
      "    PP -> DT PP CC PP\n",
      "    ADJP -> JJ\n",
      "    ADJP -> ADJP PP\n",
      "    ADJP -> JJR\n",
      "    ADVP -> RB\n",
      "    SBAR -> SBAR CC SBAR\n",
      "    SBAR -> WHNP S\n",
      "    SBAR -> WHADVP S\n",
      "    WHNP -> WDT\n",
      "    WHADVP -> WRB\n",
      "    WHADJP -> RB WRB JJ\n",
      "    PRT -> RP\n",
      "    PRN -> SINV\n",
      "    SINV -> VP NP\n",
      "    NN -> 'linguistics'\n",
      "    NNS -> 'linguistics'\n",
      "    VBZ -> 'is'\n",
      "    DT -> 'the'\n",
      "    NN -> 'study'\n",
      "    IN -> 'of'\n",
      "    NN -> 'language'\n",
      "    NNS -> 'people'\n",
      "    WP -> 'who'\n",
      "    VBP -> 'are'\n",
      "    VBP -> 'study'\n",
      "    VBN -> 'called'\n",
      "    NN -> 'linguists'\n",
      "    JJ -> 'many'\n",
      "    VBP -> 'compare'\n",
      "    NNS -> 'languages'\n",
      "    TO -> 'to'\n",
      "    VB -> 'find'\n",
      "    JJ -> 'similar'\n",
      "    NNS -> 'properties'\n",
      "    EX -> 'there'\n",
      "    VBP -> 'are'\n",
      "    JJ -> 'many'\n",
      "    NNS -> 'ways'\n",
      "    VB -> 'use'\n",
      "    DT -> 'every'\n",
      "    NN -> 'day'\n",
      "    IN -> 'for'\n",
      "    JJ -> 'many'\n",
      "    NNS -> 'centuries'\n",
      "    RBS -> 'most'\n",
      "    JJ -> 'linguistic'\n",
      "    NN -> 'work'\n",
      "    VBD -> 'was'\n",
      "    JJ -> 'literary'\n",
      "    JJ -> 'philology'\n",
      "    NNS -> 'theorists'\n",
      "    VBP -> 'study'\n",
      "    NN -> 'use'\n",
      "    IN -> 'of'\n",
      "    IN -> 'in'\n",
      "    NN -> 'literature'\n",
      "    NN -> 'analysis'\n",
      "    NN -> 'discourse'\n",
      "    JJ -> 'entire'\n",
      "    NNS -> 'conversations'\n",
      "    CC -> 'or'\n",
      "    NNS -> 'texts'\n",
      "    DT -> 'some'\n",
      "    JJ -> 'applied'\n",
      "    CC -> 'and'\n",
      "    VBP -> 'use'\n",
      "    VB -> 'do'\n",
      "    VBP -> 'do'\n",
      "    NNS -> 'things'\n",
      "    CD -> 'one'\n",
      "    NN -> 'part'\n",
      "    IN -> 'of'\n",
      "    JJ -> 'historical'\n",
      "    NN -> 'history'\n",
      "    NNS -> 'words'\n",
      "    RB -> 'etymology'\n",
      "    NN -> 'etymology'\n",
      "    RB -> 'first'\n",
      "    JJ -> 'first'\n",
      "    VBN -> 'used'\n",
      "    VB -> 'talk'\n",
      "    IN -> 'behind'\n",
      "    IN -> 'about'\n",
      "    DT -> 'a'\n",
      "    NN -> 'example'\n",
      "    NN -> 'word'\n",
      "    NN -> 'meaning'\n",
      "    NN -> 'semiotics'\n",
      "    JJ -> 'general'\n",
      "    NNS -> 'signs'\n",
      "    NNS -> 'symbols'\n",
      "    DT -> 'both'\n",
      "    IN -> 'within'\n",
      "    IN -> 'without'\n",
      "    DT -> 'another'\n",
      "    VBN -> 'involved'\n",
      "    VBG -> 'understanding'\n",
      "    NNS -> 'languages'\n",
      "    NN -> 'society'\n",
      "    NN -> 'world'\n",
      "    WDT -> 'that'\n",
      "    VBZ -> 'aims'\n",
      "    RP -> 'out'\n",
      "    VBZ -> 'works'\n",
      "    NN -> 'mind'\n",
      "    VBN -> 'known'\n",
      "    IN -> 'as'\n",
      "    NN -> 'psycholinguistics'\n",
      "    WRB -> 'how'\n",
      "    NNP -> 'Mccarthy'\n",
      "    NNP -> 'John'\n",
      "    VBD -> 'came'\n",
      "    RP -> 'up'\n",
      "    IN -> 'with'\n",
      "    NN -> 'name'\n",
      "    JJ -> 'artificial'\n",
      "    NN -> 'intelligence'\n",
      "    CD -> '1955'\n",
      "    NN -> 'term'\n",
      "    VBZ -> 'means'\n",
      "    NN -> 'programme'\n",
      "    WDT -> 'which'\n",
      "    VBZ -> 'mimics'\n",
      "    JJ -> 'human'\n",
      "    NN -> 'cognition'\n",
      "    NNS -> 'tools'\n",
      "    NNS -> 'approaches'\n",
      "    VBP -> 'have'\n",
      "    VBN -> 'been'\n",
      "    VBN -> 'tried'\n",
      "    PRP -> 'they'\n",
      "    IN -> 'on'\n",
      "    PRP -> 'their'\n",
      "    JJ -> 'own'\n",
      "    VBG -> 'being'\n",
      "    VBN -> 'encoded'\n",
      "    NNS -> 'commands'\n",
      "    PRP -> 'it'\n",
      "    RB -> 'also'\n",
      "    NN -> 'field'\n",
      "    VBZ -> 'tries'\n",
      "    VB -> 'make'\n",
      "    NNS -> 'computers'\n",
      "    JJ -> 'smart'\n",
      "    NNP -> 'ai'\n",
      "    NNP -> 'humanized'\n",
      "    VBZ -> 'shows'\n",
      "    NNS -> 'characteristics'\n",
      "    DT -> 'all'\n",
      "    NNS -> 'types'\n",
      "    NNS -> 'competencies'\n",
      "    NN -> 'research'\n",
      "    RB -> 'really'\n",
      "    VBD -> 'started'\n",
      "    NN -> 'conference'\n",
      "    IN -> 'at'\n",
      "    NNP -> 'Dartmouth'\n",
      "    NNP -> 'College'\n",
      "    CD -> '1956'\n",
      "    RB -> 'unfortunately'\n",
      "    NNS -> 'researchers'\n",
      "    VBD -> 'underestimated'\n",
      "    RB -> 'just'\n",
      "    JJ -> 'hard'\n",
      "    NNS -> 'problems'\n",
      "    VBD -> 'were'\n",
      "    NN -> 'funding'\n",
      "    VBN -> 'cut'\n",
      "    VBG -> 'starting'\n",
      "    DT -> 'an'\n",
      "    NN -> 'winter'\n",
      "    WRB -> 'where'\n",
      "    JJ -> 'little'\n",
      "    VBN -> 'done'\n",
      "    VBG -> 'searching'\n",
      "    NNS -> 'databases'\n",
      "    VBG -> 'doing'\n",
      "    NNS -> 'calculations'\n",
      "    JJR -> 'better'\n",
      "    IN -> 'than'\n",
      "    JJ -> 'monthlong'\n",
      "    NN -> 'brainstorming'\n",
      "    NN -> 'session'\n",
      "    VBN -> 'attended'\n",
      "    IN -> 'by'\n",
      "    NNS -> 'interests'\n",
      "    VBZ -> 'studies'\n",
      "    NN -> 'sociolinguistics'\n",
      "    VBP -> 'change'\n",
      "    IN -> 'over'\n",
      "    NN -> 'past'\n",
      "    NN -> 'time'\n",
      "    JJ -> 'forensic'\n",
      "    NN -> 'investigations'\n",
      "    NN -> 'crime'\n",
      "    JJ -> 'computational'\n",
      "    VB -> 'help'\n",
      "    VB -> 'understand'\n",
      "    NN -> 'structured'\n",
      "    VBN -> 'said'\n",
      "    JJ -> 'theoretical'\n",
      "    VBD -> 'began'\n",
      "    NN -> 'india'\n",
      "    NN -> 'panini'\n",
      "    JJ -> '5th'\n",
      "    NN -> 'bc'\n",
      "    NN -> 'century'\n",
      "    NN -> 'grammarian'\n",
      "    VBD -> 'wrote'\n",
      "    CD -> '3959'\n",
      "    NNS -> 'rules'\n",
      "    JJ -> 'sanskrit'\n",
      "    NN -> 'plato'\n",
      "    NN -> 'grammar'\n",
      "    JJ -> 'western'\n",
      "    NN -> 'philosopher'\n",
      "    VB -> 'write'\n",
      "    NN -> 'semantics'\n",
      "    VBP -> 'consider'\n",
      "    NN -> 'danger'\n",
      "    NN -> 'humanity'\n",
      "    IN -> 'if'\n",
      "    VBZ -> 'continues'\n",
      "    VB -> 'progress'\n",
      "    PRP -> 'its'\n",
      "    JJ -> 'current'\n",
      "    NN -> 'pace'\n",
      "    IN -> 'to'\n"
     ]
    }
   ],
   "source": [
    "print(grmmr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8312066d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-d8fcbf9a1b7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrmmr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_one\u001b[1;34m(grammar, item, depth)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNonterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproductions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_one\u001b[1;34m(grammar, item, depth)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNonterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproductions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_one\u001b[1;34m(grammar, item, depth)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNonterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproductions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_one\u001b[1;34m(grammar, item, depth)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNonterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproductions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_one\u001b[1;34m(grammar, item, depth)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNonterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproductions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\generate.py\u001b[0m in \u001b[0;36m_generate_all\u001b[1;34m(grammar, items, depth)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfrag2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_generate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                     \u001b[1;32myield\u001b[0m \u001b[0mfrag1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrag2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"maximum recursion depth exceeded\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.parse.generate import generate\n",
    "import random\n",
    "sentences = []\n",
    "for sentence in generate(grmmr, n=1000, depth = 5):\n",
    "    sentences.append(sentence)\n",
    "random.choices(sentences, k = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0266fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = []\n",
    "with open('S.txt') as f:\n",
    "    for line in f:\n",
    "        S.append(' '.join(re.findall('\\'(\\w*)\\'',line)) + '\\n')\n",
    "with open('sentences.txt','w') as f:\n",
    "    for line in S:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d87f7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexical rules:  214\n",
      "Phrase rules:  115\n",
      "329\n"
     ]
    }
   ],
   "source": [
    "lex = 0\n",
    "p = 0\n",
    "with open('tstgrmmr.txt') as f:\n",
    "    for line in f:\n",
    "        if re.match('.*\\'(\\w*)\\'',line):\n",
    "\n",
    "            lex += 1\n",
    "        else:\n",
    "            p += 1\n",
    "print(\"lexical rules: \",lex)\n",
    "print(\"Phrase rules: \",p)\n",
    "print(lex+p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b299888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('sentences.txt') as f:\n",
    "    with open('list.txt','w') as f1:\n",
    "        f1.write('\\begin{center}\\n\\begin{itemize}')\n",
    "        for line in f:\n",
    "            f1.write(f'\\item[] {line}')\n",
    "        f1.write('\\end{itemize}\\n\\end{center}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48541e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
